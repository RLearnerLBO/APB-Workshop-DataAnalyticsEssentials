getwd()
#
df <- USArrests
#K Means
#https://uc-r.github.io/kmeans_clustering
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
df <- USArrests
df <- na.omit(df)
df <- scale(df)
distance <- get_dist(df)
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
k2 <- kmeans(df, centers = 2, nstart = 25)
fviz_cluster(k2, data = df)
df %>%
as_tibble() %>%
mutate(cluster = k2$cluster,
state = row.names(USArrests)) %>%
ggplot(aes(UrbanPop, Murder, color = factor(cluster), label = state)) +
geom_text()
k3 <- kmeans(df, centers = 3, nstart = 25)
k4 <- kmeans(df, centers = 4, nstart = 25)
k5 <- kmeans(df, centers = 5, nstart = 25)
# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = df) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = df) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df) + ggtitle("k = 5")
library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)
set.seed(123)
fviz_nbclust(df, kmeans, method = "wss")
# Compute k-means clustering with k = 4
set.seed(123)
final <- kmeans(df, 4, nstart = 25)
print(final)
fviz_cluster(final, data = df)
fviz_cluster(final, data = df)
#Auto-encoder
# autoencoder in keras
library(keras)
# set training data
###
model3 <- keras_model_sequential()
Y
install_tensorflow()
library(keras)
install_tensorflow()
install_tensorflow() conda activate r-reticulate
conda activate r-reticulate
install.packages("reticulate")
install_tensorflow()
library(keras)
install_tensorflow()
install.packages("tensorflow")
install.packages("tensorflow")
library(tensorflow)
install_tensorflow()
library(tensorflow)
install_tensorflow()
install.packages("Rcpp")
install.packages("Rcpp")
install.packages("Rcpp")
library(tensorflow)
install_tensorflow()
library(tensorflow)
install_tensorflow()
library(keras)
install_keras()
install_keras(tensorflow = "2.1")
install_keras()
library(keras)
install_keras()
#
library(tidyverse)  # data manipulation
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
df <- USArrests
df <- na.omit(df)
df <- scale(df)
library(keras)
#
# T-SNE
library(Rtsne)
library(ggplot2)
data(iris)
iris.unique <- as.matrix(iris[,1:4])
iris.unique <- iris.unique[!duplicated(iris.unique),]
iris.rtsne <- Rtsne(iris.unique, dims = 2, initial_dims = 50, perplexity = 30)
install.packages(c("colorspace", "curl", "googledrive", "isoband", "Rcpp", "spatstat.geom", "xfun"))
install.packages(c("Matrix", "mgcv"), lib="C:/Program Files/R/R-4.1.0/library")
install.packages(c("colorspace", "curl", "googledrive", "isoband", "Rcpp", "spatstat.geom", "xfun"))
install.packages(c("colorspace", "curl", "googledrive", "isoband", "Rcpp", "spatstat.geom", "xfun"))
install.packages(c("colorspace", "curl", "googledrive", "isoband", "Rcpp", "spatstat.geom", "xfun"))
#
# T-SNE
library(Rtsne)
library(ggplot2)
data(iris)
iris.unique <- as.matrix(iris[,1:4])
iris.unique <- iris.unique[!duplicated(iris.unique),]
iris.rtsne <- Rtsne(iris.unique, dims = 2, initial_dims = 50, perplexity = 30)
plotdata <- data.frame(iris.umap$layout)
View(iris.rtsne)
iris.rtsne$Y
plotdata <- as.data.frame(iris.rtsne$Y)
View(plotdata)
plotdata <- as.data.frame(iris.rtsne$Y)
ggplot(plotdata, aes(x=V1,y=V2)) + geom_point(aes(color=iris$Species))
data(iris)
iris.unique <- iris.unique[!duplicated(iris.unique),]
label <- iris.unique$Species
View(iris.unique)
data(iris)
iris
iris.unique <- iris.unique[!duplicated(iris.unique),]
label <- iris.unique$Species
label <- iris.uniquep[,5]
label <- iris.unique[,5]
iris.unique <- iris[!duplicated(iris),]
label <- iris.unique[,5]
iris.unique <- as.matrix(iris[,1:4])
iris.unique <- iris[!duplicated(iris),]
label <- iris.unique[,5]
iris.unique <- as.matrix(iris.unique[,1:4])
iris.rtsne <- Rtsne(iris.unique, dims = 2, initial_dims = 50, perplexity = 30)
plotdata <- as.data.frame(iris.rtsne$Y)
ggplot(plotdata, aes(x=V1,y=V2)) + geom_point(aes(color=label))
#https://fderyckel.github.io/machinelearningwithr/knnchapter.html
df <- data(iris) ##load data
head(iris) ## see the studcture
##Generate a random number that is 90% of the total number of rows in dataset.
ran <- sample(1:nrow(iris), 0.9 * nrow(iris))
##the normalization function is created
nor <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
##Run nomalization on first 4 coulumns of dataset because they are the predictors
iris_norm <- as.data.frame(lapply(iris[, c(1, 2, 3, 4)], nor))
summary(iris_norm)
plor(iris_norm)
plot(iris_norm)
iris_train <- iris_norm[ran, ]
##extract testing set
iris_test <- iris_norm[-ran, ]
##extract 5th column of train dataset because it will be used as 'cl' argument in knn function.
iris_target_category <- iris[ran, 5]
##extract 5th column if test dataset to measure the accuracy
iris_test_category <- iris[-ran, 5]
##load the package class
library(class)
##run knn function
pr <- knn(iris_train, iris_test, cl = iris_target_category, k = 13)
##create confusion matrix
tab <- table(pr, iris_test_category)
accuracy <- function(x) {
sum(diag(x) / (sum(rowSums(x)))) * 100
}
accuracy(tab)
tab
##create confusion matrix
tab <- table(model_knn, iris_test_category)
##run knn function
model_knn <- knn(iris_train, iris_test, cl = iris_target_category, k = 13)
##create confusion matrix
tab <- table(model_knn, iris_test_category)
tab
install.packages("ISLR")
library(ISLR)
# Load the party package. It will automatically load other
# dependent packages.
library(party)
# Create the input data frame.
input.dat <- readingSkills[c(1:105),]
# Give the chart file a name.
png(file = "decision_tree.png")
# Save the file.
dev.off()
# Create the tree.
output.tree <- ctree(
nativeSpeaker ~ age + shoeSize + score,
data = input.dat)
# Plot the tree.
plot(output.tree)
View(input.dat)
#
#Convolutional
library(tensorflow)
library(keras)
library(stringr)
library(readr)
library(purrr)
library(caret)
library(e1071)
cifar <- dataset_cifar10()
train_data <- scale(cifar$train$x)
dim(train_data) <- c(50000,32,32,3)
test_data <- scale(cifar$test$x)
dim(test_data) <- c(10000,32,32,3)
train_label <- as.numeric(cifar$train$y)
dim(train_label) <- c(50000)
test_label <- as.numeric(cifar$test$y)
dim(test_label) <- c(10000)
class_names <- c('airplane', 'automobile', 'bird', 'cat', 'deer',
'dog', 'frog', 'horse', 'ship', 'truck')
index <- 1:30
par(mfcol = c(5,6), mar = rep(1, 4), oma = rep(0.2, 4))
cifar$train$x[index,,,] %>%
purrr::array_tree(1) %>%
purrr::set_names(class_names[cifar$train$y[index] + 1]) %>%
purrr::map(as.raster, max = 255) %>%
purrr::iwalk(~{plot(.x); title(.y)})
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = "relu", input_shape = c(32,32,3)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2,2))
summary(model)
model %>%
layer_flatten() %>%
layer_dense(units = 256, activation = "relu") %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 10, activation = "softmax")
summary(model)
model %>% compile(
optimizer = "adam",
loss = "sparse_categorical_crossentropy",
metrics = "accuracy"
)
history <- model %>%
fit(
x = train_data, y = train_label,
epochs = 20,
validation_split=0.2,
use_multiprocessing=TRUE
)
#Auto-encoder
#https://www.r-bloggers.com/2018/07/pca-vs-autoencoders-for-dimensionality-reduction/
suppressPackageStartupMessages(library(DAAG))
head(ais)
# standardise
minmax <- function(x) (x - min(x))/(max(x) - min(x))
x_train <- apply(ais[,1:11], 2, minmax)
# autoencoder in keras
suppressPackageStartupMessages(library(keras))
# set training data
x_train <- as.matrix(x_train)
# autoencoder in keras
library(keras)
# set training data
###
model3 <- keras_model_sequential()
model3 %>%
layer_dense(units = 6, activation = "tanh", input_shape = ncol(x_train)) %>%
layer_dense(units = 3, activation = "tanh", name = "bottleneck") %>%
layer_dense(units = 6, activation = "tanh") %>%
layer_dense(units = ncol(x_train))
# summary of model
summary(model3)
# compile model
model3 %>% compile(
loss = "mean_squared_error",
optimizer = "adam"
)
# fit model
model3 %>% fit(
x = x_train,
y = x_train,
verbose = 0,
epochs = 100,
batch_size = 2
)
# evaluate the performance of the model
mse.ae2 <- evaluate(model3, x_train, x_train)
mse.ae2
# exgtract the bottleneck layer
intermediate_layer_model <- keras_model(inputs = model3$input, outputs = get_layer(model3, "bottleneck")$output)
intermediate_output <- predict(intermediate_layer_model, x_train)
# plot the reduced dat set
aedf3 <- data.frame(node1 = intermediate_output[,1], node2 = intermediate_output[,2], node3 = intermediate_output[,3])
ae_plotly <- plot_ly(aedf3, x = ~node1, y = ~node2, z = ~node3, color = ~ais$sex) %>% add_markers()
ae_plotly <- plot_ly(aedf3, x = ~node1, y = ~node2, z = ~node3, color = ~ais$sex)
library(plotly
)
library(plotly)
ae_plotly <- plot_ly(aedf3, x = ~node1, y = ~node2, z = ~node3, color = ~ais$sex) %>% add_markers()
ae_plotly
